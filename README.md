# üöï Case ‚Äì Pipeline de Dados com PySpark e Databricks

Este projeto foi desenvolvido como parte de um **case t√©cnico para a posi√ß√£o de Data Engineer**, utilizando o **Databricks Free Edition** e versionado com **GitHub**.

A solu√ß√£o foca na ingest√£o, transforma√ß√£o e an√°lise de dados de t√°xis de Nova York com uma arquitetura em camadas (Bronze, Silver, Gold), utilizando **PySpark**, **Delta Lake** e armazenamento em **S3** via **Unity Catalog**.

---

## üéØ Objetivo

-Ingerir dados das corridas de Yellow Taxis de NY (jan a mai/2023).
-Criar uma estrutura de Data Lake com camadas Bronze ‚Üí Silver ‚Üí Gold.
-Criar tabelas no Unity Catalog.
-Disponibilizar os dados via SQL.

---

## üß† Dados Utilizados

Fonte oficial: [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

Foram utilizados os dados de **Janeiro a Julho de 2023**, considerando a observa√ß√£o na documenta√ß√£o de que os dados podem levar at√© **dois meses** para serem disponibilizados. Os meses 06 e 07 foram inclu√≠dos na ingest√£o, mas **apenas os meses de jan-mai foram utilizados nas an√°lises finais**, conforme exigido no desafio. Um filtro de meses foi aplicado diretamente no c√≥digo

---

## üìö Perguntas Respondidas

1. Qual a **m√©dia do valor total (`total_amount`)** recebido por m√™s?
2. Qual a **m√©dia de passageiros (`passenger_count`) por hora** no m√™s de maio?

---

## üß∞ Tecnologias Utilizadas

- [x] **Databricks Free Edition**
- [x] **PySpark**
- [x] **Delta Lake**
- [x] **Amazon S3** (via External Location)
- [x] **Unity Catalog**
- [x] **GitHub** (controle de vers√£o)

---

## üìÇ Arquitetura
Camadas utilizadas:

- **Bronze**: Armazena os arquivos .parquet sem modifica√ß√µes.
- **Silver**: Aplica sele√ß√£o de colunas, padroniza nomes e cria a particao `year_month`.
- **Gold**: Agrega m√©tricas e disponibiliza os dados finais para an√°lise.

![image](https://github.com/user-attachments/assets/aaab2299-d372-42b2-9d33-652e35f95d26)


> ‚úâÔ∏è Os dados n√£o foram modificados em conte√∫do, apenas **padronizados os nomes das colunas** (para camelCase) e feito o cast de tipos adequados.


---

## üìÅ Estrutura do Projeto

```
ifood-case/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_requests.py          # Download dos arquivos .parquet
‚îÇ   ‚îú‚îÄ‚îÄ data_transformers.py      # Transforma√ß√µes (ingest√£o, parti√ß√£o, colunas)
‚îÇ   ‚îú‚îÄ‚îÄ save_data.py              # Escrita no S3 / Escrita no Unity Catalog
‚îÇ   ‚îî‚îÄ‚îÄ data_analysis.py          # An√°lises com PySpark
‚îÇ
‚îú‚îÄ‚îÄ etl_bronze.ipynb              # Baixa dados e salva na camada Bronze
‚îú‚îÄ‚îÄ etl_silver.ipynb              # Filtra e transforma para Silver
‚îú‚îÄ‚îÄ etl_gold.ipynb                # Agrega e salva resultados finais (Gold)
‚îÇ
‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îî‚îÄ‚îÄ perguntas.ipynb           # Consulta dos resultados anal√≠ticos
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚ñ∂Ô∏è Como Executar no Databricks (Free Edition)

### 1. Clone e use direto no Databricks

- Clone esse reposit√≥rio
- No Databricks, v√° em **Repos > Add Repo > Git URL** e insira:

```bash
https://github.com/<seuGitHub>/data-engineer-case
```
<img width="805" alt="image" src="https://github.com/user-attachments/assets/22113e98-7fb3-4044-b732-c89a7c2b4abc" />

### 2. Configure uma External Location no Unity Catalog

- Ex. de nome para o Bucket: `s3://<seunome>-ifood-case/`
- Role IAM com permiss√µes de leitura/escrita no bucket

![image](https://github.com/user-attachments/assets/a670d666-92cc-4ee6-a7c6-77132c6bf880)

##### Obs.: para instalar as dependencias √© s√≥ usar `pip install -r requirements.txt`

---

## üö¶ Ordem de Execu√ß√£o

1. `etl_bronze.ipynb` ‚Äì Baixa arquivos `.parquet` e salva dados brutos no S3  
2. `etl_silver.ipynb` ‚Äì Seleciona colunas requeridas e adiciona parti√ß√£o  
3. `etl_gold.ipynb` ‚Äì Agrega m√©tricas e salva tabelas finais  
4. `analysis/perguntas.ipynb` ‚Äì Executa as consultas finais

Voc√™ consegue acompanhar os logs e erros da execu√ß√£o:
![image](https://github.com/user-attachments/assets/9a4ca78f-bb46-41df-9569-f0f37daeddfc)


---

## üìä Acesso aos Dados

### Tabelas criadas:
- `ifood_case.bronze.yellow_trip`
- `ifood_case.silver.yellow_trip`
- `ifood_case.gold.media_valor_mensal`
- `ifood_case.gold.media_passageiros_hora`

### Exemplo de consulta:

```sql
-- M√©dia mensal de total_amount
SELECT * FROM ifood_case.gold.media_valor_mensal;

-- M√©dia de passageiros por hora em maio
SELECT * FROM ifood_case.gold.media_passageiros_hora;
```

---

## üìä Resultados Anal√≠ticos

### M√©dia mensal de total_amount (2023-01 a 2023-05)
| year_month | media_total_amount |
|------------|--------------------|
| 2023-01    | 27.02              |
| 2023-02    | 26.9               |
| 2023-03    | 27.9               |
| 2023-04    | 28.27              |
| 2023-05    | 28.96              |

### M√©dia de passageiros por hora (maio/2023)
744 linhas correspondentes √†s combina√ß√µes de `data x hora` com m√©dias arredondadas entre 1.2 a 1.5 passageiros.

---

## üì¶ Requisitos

- Databricks Free Edition (https://www.databricks.com/learn/free-edition)
- Conta AWS com bucket S3 configurado e external location registrada
- Permiss√µes de leitura/grava√ß√£o no Unity Catalog
- Python 3.8+ com PySpark

---

## üñºÔ∏è Imagens

- Camada Bronze
  - Notebook
    ![image](https://github.com/user-attachments/assets/16099c71-0e92-40f4-a564-40b285ecafed)
    
  - Bucket
    ![image](https://github.com/user-attachments/assets/dc03d9e9-2818-429e-8e09-efe264b48f70)
    
  - Tabela
    ![image](https://github.com/user-attachments/assets/274af521-22d2-4cd1-9226-afc49f67b24a)
 
- Camada Silver
  - Notebook
    ![image](https://github.com/user-attachments/assets/7d8390e2-dac7-4f63-a559-5aad5a8e855d)
    
  - Bucket
    ![image](https://github.com/user-attachments/assets/d897a148-df38-4ca9-aae1-4c089045e723)

  - Tabela
    ![image](https://github.com/user-attachments/assets/fb3fa78e-b382-4f2f-bd41-e04844f69191)

- Camada Gold
  - Notebook
    ![image](https://github.com/user-attachments/assets/b22f1297-e71e-4f29-b787-c3af4cfe47b7)

  - Bucket
    ![image](https://github.com/user-attachments/assets/39ff1bfa-caf7-4634-9030-7666ca48c1a3)

  - Tabela
    ![image](https://github.com/user-attachments/assets/64a2e3ac-5e93-4f50-a716-e0ca3b479e35)
    
  - Perguntas
    ![image](https://github.com/user-attachments/assets/d3f07d4b-17ad-43c5-809b-c86923a0c882)



---

## ‚úÖ Status do Projeto

- [x] Ingest√£o automatizada com controle por m√≥dulo
- [x] Camadas Bronze, Silver e Gold implementadas
- [x] Tabelas em Delta criadas no Unity Catalog
- [x] Respostas anal√≠ticas obtidas via PySpark e SQL
- [x] Versionamento completo no GitHub

---

## ‚úÖ Atendimento aos Requisitos do Desafio

Este projeto cumpre integralmente os requisitos propostos no **Case T√©cnico de Data Engineer do iFood**, incluindo:

- ‚úîÔ∏è Ingest√£o dos dados de corridas de t√°xi de NY (janeiro e maio de 2023) diretamente da fonte oficial em formato Parquet;
- ‚úîÔ∏è Organiza√ß√£o em camadas no Data Lake (**Bronze**, **Silver** e **Gold**), utilizando **Delta Lake** com **particionamento** e **Unity Catalog**;
- ‚úîÔ∏è Disponibiliza√ß√£o dos dados para consumo via **SQL** diretamente no Databricks;
- ‚úîÔ∏è Implementa√ß√£o em **PySpark** com separa√ß√£o clara entre extra√ß√£o, transforma√ß√£o, an√°lise e salvamento;
- ‚úîÔ∏è Garantia das colunas obrigat√≥rias: `VendorID`, `passenger_count`, `total_amount`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`;
- ‚úîÔ∏è Respostas √†s perguntas anal√≠ticas solicitadas, dispon√≠veis no notebook [`/analysis/perguntas.ipynb`](https://github.com/WendySilva/ifood-case/blob/main/analysis/perguntas.ipynb):
  - M√©dia mensal de `total_amount` por frota.
  - M√©dia de `passenger_count` por hora, no m√™s de **maio/2023**.

---

## üì¨ Contato

Desenvolvido por [@WendySilva](https://github.com/WendySilva)  
Em caso de d√∫vidas, sugest√µes ou contribui√ß√µes, abra uma issue ou entre em contato.

[@LinkedIn](https://www.linkedin.com/in/wendysmendonca/)
