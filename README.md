# ðŸš• iFood Case â€“ Pipeline de Dados com PySpark e Databricks

Este projeto foi desenvolvido como parte de um **case tÃ©cnico para a posiÃ§Ã£o de Data Engineer no iFood**, utilizando o **Databricks Free Edition** e versionado com **GitHub**.

A soluÃ§Ã£o foca na ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise de dados de tÃ¡xis de Nova York com uma arquitetura em camadas (Bronze, Silver, Gold), utilizando **PySpark**, **Delta Lake** e armazenamento em **S3** via **Unity Catalog**.

---

## ðŸŽ¯ Objetivo

- Ingerir dados das corridas de Yellow Taxis de NY (jan a mai/2023).
- Armazenar e estruturar os dados no formato Delta Lake em camadas (Bronze â†’ Silver â†’ Gold).
- Criar tabelas no Unity Catalog.
- Responder duas perguntas analÃ­ticas usando PySpark e SQL.

---

## ðŸ“š Perguntas Respondidas

1. Qual a **mÃ©dia do valor total (`total_amount`)** recebido por mÃªs?
2. Qual a **mÃ©dia de passageiros (`passenger_count`) por hora** no mÃªs de maio?

---

## ðŸ§° Tecnologias Utilizadas

- [x] **Databricks Free Edition**
- [x] **PySpark**
- [x] **Delta Lake**
- [x] **Amazon S3** (via External Location)
- [x] **Unity Catalog**
- [x] **GitHub** (controle de versÃ£o)

---

## ðŸ“ Estrutura do Projeto

```
ifood-case/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_requests.py          # Download dos arquivos .parquet
â”‚   â”œâ”€â”€ data_transformers.py      # TransformaÃ§Ãµes (ingestÃ£o, partiÃ§Ã£o, colunas)
â”‚   â”œâ”€â”€ save_data.py              # Escrita no S3 / Escrita no Unity Catalog
â”‚   â””â”€â”€ data_analysis.py          # AnÃ¡lises com PySpark
â”‚
â”œâ”€â”€ etl_bronze.ipynb              # Baixa dados e salva na camada Bronze
â”œâ”€â”€ etl_silver.ipynb              # Filtra e transforma para Silver
â”œâ”€â”€ etl_gold.ipynb                # Agrega e salva resultados finais (Gold)
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ perguntas.ipynb           # Consulta dos resultados analÃ­ticos
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â–¶ï¸ Como Executar no Databricks (Free Edition)

### 1. Clone e use direto no Databricks

No Databricks, vÃ¡ em **Repos > Add Repo > Git URL** e insira:

```bash
https://github.com/<seuGitHub>/ifood-case
```
### 2. Configure uma External Location no Unity Catalog

- Bucket: `s3://<seunome>-ifood-case/`
- Role IAM com permissÃµes de leitura/escrita no bucket

##### Obs.: NÃ£o Ã© necessÃ¡rio instalar dependÃªncias, pois vocÃª conseguirÃ¡ executar diretamente no Databricks.

---

## ðŸš¦ Ordem de ExecuÃ§Ã£o

1. `etl_bronze.ipynb` â€“ Baixa arquivos `.parquet` e salva dados brutos no S3  
2. `etl_silver.ipynb` â€“ Seleciona colunas requeridas e adiciona partiÃ§Ã£o  
3. `etl_gold.ipynb` â€“ Agrega mÃ©tricas e salva tabelas finais  
4. `analysis/perguntas.ipynb` â€“ Executa as consultas finais

---

## ðŸ“Š Acesso aos Dados

### Tabelas criadas:
- `ifood_case.bronze.yellow_trip`
- `ifood_case.silver.yellow_trip`
- `ifood_case.gold.media_valor_mensal`
- `ifood_case.gold.media_passageiros_hora`

### Exemplo de consulta:

```sql
-- MÃ©dia mensal de total_amount
SELECT * FROM ifood_case.gold.media_valor_mensal;

-- MÃ©dia de passageiros por hora em maio
SELECT * FROM ifood_case.gold.media_passageiros_hora;
```

---

## ðŸ“¦ Requisitos

- Databricks Free Edition (https://www.databricks.com/learn/free-edition)
- Conta AWS com bucket S3 configurado e external location registrada
- PermissÃµes de leitura/gravaÃ§Ã£o no Unity Catalog
- Python 3.8+ com PySpark

---

## âœ… Status do Projeto

- [x] IngestÃ£o automatizada com controle por mÃ³dulo
- [x] Camadas Bronze, Silver e Gold implementadas
- [x] Tabelas em Delta criadas no Unity Catalog
- [x] Respostas analÃ­ticas obtidas via PySpark e SQL
- [x] Versionamento completo no GitHub

---

## ðŸ“¬ Contato

Desenvolvido por [@WendySilva](https://github.com/WendySilva)  
Em caso de dÃºvidas, sugestÃµes ou contribuiÃ§Ãµes, abra uma issue ou entre em contato.

[@LinkedIn](https://www.linkedin.com/in/wendysmendonca/)
